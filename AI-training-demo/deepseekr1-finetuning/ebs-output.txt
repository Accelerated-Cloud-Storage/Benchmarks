(.venv) [ec2-user@ip-172-31-25-0 deepseekr1-finetuning]$ python train.py --output_dir ~/localDir --cache_dir ~/localDir --d
ataset_name wikitext --dataset_config wikitext-2-v1 --max_train_samples 1000

=== Starting Training Pipeline ===

Step 1: Loading tokenizer and model...
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Time taken: 6.62 seconds

Step 2: Loading dataset...
Time taken: 1.30 seconds

Step 3: Tokenizing dataset...
Parameter 'function'=<function main.<locals>.tokenize_function at 0x7fe7096bfc10> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map: 100%|███████████████████████████████████████████████████████████████████| 4358/4358 [00:00<00:00, 21892.64 examples/s]
Map: 100%|█████████████████████████████████████████████████████████████████| 36718/36718 [00:01<00:00, 21762.47 examples/s]
Map: 100%|███████████████████████████████████████████████████████████████████| 3760/3760 [00:00<00:00, 23874.85 examples/s]
Time taken: 2.05 seconds

Step 4: Grouping text into blocks...
Map: 100%|████████████████████████████████████████████████████████████████████| 4358/4358 [00:00<00:00, 6690.38 examples/s]
Map: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 7210.09 examples/s]
Map: 100%|████████████████████████████████████████████████████████████████████| 3760/3760 [00:00<00:00, 6609.31 examples/s]
Time taken: 1.37 seconds

Step 5: Setting up data collator...
Time taken: 0.00 seconds

Step 6: Setting up training arguments...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ec2-user/Benchmarks/AI-training-demo/deepseekr1-finetuning/train.py:166: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Time taken: 1.85 seconds

Step 7: Starting training...
{'train_runtime': 49.4125, 'train_samples_per_second': 1.315, 'train_steps_per_second': 0.162, 'train_loss': 3.1208393573760986, 'epoch': 0.97}
100%|████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:49<00:00,  6.18s/it]
Training time: 49.55 seconds

Step 8: Saving final model...
Time taken: 9.86 seconds

=== Training Pipeline Completed ===
Total time taken: 72.59 seconds (0.02 hours)
Training time: 49.55 seconds (0.01 hours)